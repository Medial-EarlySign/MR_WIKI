## Creating the Process Pipeline

The next step in the ETL workflow is to build a processing pipeline using the `prepare_final_signals` function. This function takes the `data_fetcher` you created in the previous step and applies your custom processing logic to the data.

### Step-by-Step Guide
1. **Import Necessary Functions**: Start by importing the `prepare_final_signals` function and your custom data_fetcher.
```python
from ETL_Infra.etl_process import prepare_final_signals
from parser import generic_file_fetcher
```
2. **Define a Working Directory**: Set a WORK_DIR where the ETL process will store test results and output files.
```python
WORK_DIR = "..."  # Specify your working directory path
```

3. **Run the Pipeline**: Call the prepare_final_signals function to execute the pipeline.

```python
prepare_final_signals(
  generic_file_fetcher("^demo.*"), # The data fetcher for files starting with "demo"
  WORK_DIR,
  "demographic", # The name of this processing pipeline
  batch_size=0, # Process all files in a single batch
  override="n" # Skip if the process has already completed successfully
)
```

In this example, the pipeline will process all files with names starting with "demo", organize them under the "demographic" pipeline, and read all files in a single batch (`batch_size=0`). The `override='n'` argument ensures that the process doesn't re-run if it has already been completed.

### Defining Processing Logic

The pipeline will automatically look for a corresponding processing script to handle the data. 
The name of this script is derived from the pipeline name (e.g., `demographic.py`). This script should be placed in a folder named `signal_processings` alongside your main code.

Within this script, your processing logic will take a DataFrame df and transform it into the required output format.

**A Note on Signal Processing Hierarchy**: The system uses a hierarchical approach to find the right processing script. It will first look for a script named after the specific signal (e.g., `Hemoglobin.py`). If that doesn't exist, it will look for a script that handles a group of signals (e.g., `labs.py`). This allows you to write a single script to process multiple related signals. This will only happen if the data returned from the `data_fetcher` already contains a "signal" column. 
The order and the groups each signal belongs too defined in the signals definition file in [here](https://github.com/Medial-EarlySign/MR_Tools/blob/main/RepoLoadUtils/common/ETL_Infra/rep_signals/general.signals) - column number 5 for each signal:

* We can repeat that process multiple times for each data type.
* We should use `start_write_batch` parameter if we have multiple pipelines writing to the same output signals. We will need to controls a numeric batch index for the output to avoid overide. 

### Required Output Format:

The final DataFrame must contain the following columns:

* `pid`: The patient identifier (already required from the data fetching step)
* `signal`: The name of the signal (e.g., 'GENDER', 'Hemoglobin'). This must be a recognized signal. The system will provide an error with instructions if a signal is unknown (How to define a new signal if needed).
* `time_X`: Time channels, where X is an index. These should be stored as integers in YYYYMMDD format.
* `value_X`: Value channels, where X is an index. These can be float, integer, or string (for categorical data)

Any other columns will be ignored during the final loading process. The output will be batched based on the signal column, and the system will run format tests to catch missing or incorrect columns

For more detailed infromation please refer to [Processing Unit Tutorial](ETL%20Processing%20Code%20Unit%20Tutorial)

### Example Processing Code

Here's an example of the code you would write inside the `demographic.py` file to process a DataFrame and create a `GENDER` signal.

```python
# Assuming 'df' is the input DataFrame from the data fetcher
# The goal is to generate a 'GENDER' signal from the 'Sex' column

# The output DataFrame should have these columns:
#   pid
#   signal
#   value_0 (string categorical, e.g., 'Male', 'Female')

# 1. Select and rename columns
df = df[['pid', 'Sex']].rename(columns={'Sex': 'value_0'})

# 2. Add the 'signal' column with the signal name
df['signal'] = 'GENDER'

# 3. Standardize the values
df.loc[df['value_0'] == 'male', 'value_0'] = 'Male'
df.loc[df['value_0'] == 'female', 'value_0'] = 'Female'

# 4. Finalize the DataFrame by keeping only required columns and removing duplicates
df = df[['pid', 'signal', 'value_0']].drop_duplicates().reset_index(drop=True)

# The 'df' is now ready for the next stage of the pipeline
```

## Next Step: Prepare Dicts if needed

Once you have you finished, follow [this guide](ETL%20Manager%20Process.md#3-optional-prepare-special-client-dictionaries) to finalize the loading